@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2025-01-01},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\NFJWTQS6\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@misc{trabucco2023effectivedataaugmentationdiffusion,
      title={Effective Data Augmentation With Diffusion Models}, 
      author={Brandon Trabucco and Kyle Doherty and Max Gurinas and Ruslan Salakhutdinov},
      year={2023},
      eprint={2302.07944},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.07944}, 
}

@misc{tormos2024dataaugmentationdiffusionmodels,
      title={Data Augmentation with Diffusion Models for Colon Polyp Localization on the Low Data Regime: How much real data is enough?}, 
      author={Adrian Tormos and Blanca Llauradó and Fernando Núñez and Axel Romero and Dario Garcia-Gasulla and Javier Béjar},
      year={2024},
      eprint={2411.18926},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.18926}, 
}

@misc{hoClassifierFreeDiffusionGuidance2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12598},
  url = {http://arxiv.org/abs/2207.12598},
  urldate = {2025-01-01},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\FGCI42AI\Ho et Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf}
}


@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.05233},
  url = {http://arxiv.org/abs/2105.05233},
  urldate = {2025-01-01},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128{\texttimes}128, 4.59 on ImageNet 256{\texttimes}256, and 7.72 on ImageNet 512{\texttimes}512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256{\texttimes}256 and 3.85 on ImageNet 512{\texttimes}512. We release our code at https://github.com/openai/guided-diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\TLETAXYB\Dhariwal et Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf}
}

@misc{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  year = {2021},
  month = feb,
  number = {arXiv:2102.09672},
  eprint = {2102.09672},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09672},
  url = {http://arxiv.org/abs/2102.09672},
  urldate = {2025-01-09},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive loglikelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/ openai/improved-diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\HXRQB9LM\Nichol et Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf}
}

@misc{hoClassifierFreeDiffusionGuidance2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12598},
  url = {http://arxiv.org/abs/2207.12598},
  urldate = {2025-01-01},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\FGCI42AI\Ho et Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf}
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}
@misc{bengio2014representationlearningreviewnew,
      title={Representation Learning: A Review and New Perspectives}, 
      author={Yoshua Bengio and Aaron Courville and Pascal Vincent},
      year={2014},
      eprint={1206.5538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1206.5538}, 
}

@misc{zhangAddingConditionalControl2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  year = {2023},
  month = nov,
  number = {arXiv:2302.05543},
  eprint = {2302.05543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.05543},
  url = {http://arxiv.org/abs/2302.05543},
  urldate = {2025-01-01},
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained textto-image diffusion models. ControlNet locks the productionready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with ``zero convolutions'' (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({$<$}50k) and large ({$>$}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction,Computer Science - Multimedia},
  file = {C:\Users\cleme\Zotero\storage\LZATVL6G\Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffusion Models.pdf}
}
