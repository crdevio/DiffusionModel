\documentclass{beamer}
\usetheme{metropolis}

% Configuration des couleurs modernes
\definecolor{myblue}{RGB}{173, 216, 230} % Bleu pastel
\definecolor{mymagenta}{RGB}{238, 130, 238} % Magenta doux
\definecolor{mywhite}{RGB}{255, 255, 255} % Blanc pur
\definecolor{mygray}{RGB}{245, 245, 245} % Gris clair
\definecolor{darkblue}{RGB}{25, 25, 112} % Bleu foncé
\definecolor{darkviolet}{RGB}{148, 0, 211} % Violet foncé
\definecolor{goldenrod}{RGB}{218, 165, 32} % Or doux

% Arrière-plan sobre et clair
\usepackage{tikz}

\addtobeamertemplate{background canvas}{}{
    \begin{tikzpicture}[remember picture, overlay]
        \fill[mywhite] (current page.south west) rectangle (current page.north east);
        \begin{scope}[blend mode=overlay]
            \shade[inner color=myblue!60, outer color=mywhite] (current page.south west) rectangle (current page.north east);
            \shade[inner color=mymagenta!60, outer color=mywhite] (current page.north east) rectangle (current page.south west);
        \end{scope}
    \end{tikzpicture}
}

% Personnalisation des couleurs dans metropolis
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{frametitle}{bg=mywhite, fg=black}
\setbeamercolor{title separator}{fg=mymagenta}
\setbeamercolor{progress bar}{fg=myblue, bg=mymagenta}
\setbeamercolor{block title}{bg=myblue, fg=black}
\setbeamercolor{block body}{bg=mywhite, fg=black}
\setbeamercolor{alerted text}{fg=darkviolet!70}

% Personnalisation des typos
\setbeamerfont{frametitle}{size=\Large,series=\bfseries}
\setbeamerfont{title}{size=\Huge,series=\bfseries}

% Apparence des transitions entre les frames
\metroset{progressbar=frametitle,numbering=fraction}

% Commande pour ajuster le séparateur du titre
\metroset{titleformat frame=smallcaps}

% Début du document
\begin{document}

\title{Diffusion Model}
\subtitle{Computer Vision - Project}
\author{Clément, Grégoire, Nathan}
\date{\today}

\maketitle

\section{Denoising Diffusion Probabilistic Models}

\begin{frame}{General Idea}
    \only<1>{Consider the set of \alert{hand-written digits $D$}. Can you give a probability distribution $q$ such that \alert{$x \sim q(x)$ ?}
    \begin{figure}
        \centering
            \includegraphics[width=0.5\textwidth]{imgs/mnist_sample_digits.png}
            \caption{Source: ludwig.ai}
        \end{figure}
    }
    \only<2->{
        Consider the set of \alert{hand-written digits $D$}. It is hard to find $q$ such that \alert{$x \sim q(x)$}, we need a \alert{clever way to sample hand-written digits}. Consider the following process:
        \begin{figure}
            \centering
            \begin{minipage}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{imgs/frame1.png}
            \end{minipage}%
            \only<3->{
                \begin{minipage}{0.2\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{imgs/frame2.png}
                \end{minipage}
                \only<4->{
                    \begin{minipage}{0.2\textwidth}
                        \centering
                        \includegraphics[width=\textwidth]{imgs/frame3.png}
                    \end{minipage}
                    \only<5->{
                    \begin{minipage}{0.2\textwidth}
                        \centering
                        \includegraphics[width=\textwidth]{imgs/frame52.png}
                    \end{minipage}
                    \only<6->{
                    \begin{minipage}{0.2\textwidth}
                        \centering
                        \includegraphics[width=\textwidth]{imgs/framelot.png}
                    \end{minipage}
                    \only<7->{
                    \begin{minipage}{0.2\textwidth}
                        \centering
                        \includegraphics[width=\textwidth]{imgs/framerandom.png}
                    \end{minipage}
                }
                }
                }
                }
                
            }
        \end{figure}
        \only<8->{
            Formally: $q(x_{t+1} \mid x_t) := \mathcal{N}(x_{t+1}; \sqrt{1 - \beta_t} x_t, \beta_t I)$ for some schedule $(\beta_t)_t$. Can we \alert{learn to reverse this process} ?
        }
    }
\end{frame}



\begin{frame}{What we want to learn}
    Given a noisy image $x_t$, we \alert{train a model to predict $x_{t-1}$}.
    \begin{figure}
        \centering
        \includegraphics[width= \textwidth]{imgs/ho_diffusion_process.png}
    \end{figure}

    \only<2->{
    \begin{itemize}
        \item{Given a \alert{data image $x_0$}, we sample $(x_t)_{1:T}$ according to $q(x_{1:T} \mid x_0) := \prod_{t=1}^T q(x_t \mid x_{t-1})$,}
        \only<3->{\item{Given a \alert{noisy image $x_t$} and $t$, we sample according to $p_\theta (x_{t-1} \mid x_t) := \mathcal{N}(x_{t-1} ; \mu_\theta(x_t,t), \Sigma_\theta (x_t,t))$}.}
    \end{itemize}
    }
\end{frame}

\begin{frame}{Decreasing training data generation cost}
    Given a data image $x_0$, compute $x_t$ takes \alert{$t$ sampling on $q$}. But a \alert{simple trick}, allows to do only one.
    \uncover<2->{ 
        
    Remember that $q(x_{t+1} \mid x_t) := \mathcal{N}(x_{t+1}; \sqrt{1 - \beta_t} x_t, \beta_t I)$. Let $\alert{\alpha_t} = 1 - \beta_t$ and $\alert{\bar{\alpha_t}} = \prod_{i=1}^t \alpha_i$.
    \only<3-7>{
    \begin{align}
    x_t &= \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1} \nonumber\\
    \uncover<4->{
    &= \sqrt{\alpha_t} \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t} \sqrt{1 - \alpha_t} \epsilon_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1} \nonumber\\
    \uncover<6->{
    &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1}) + 1 - \alpha_t} \bar{\epsilon_t} \label{eq:toexplain}\\ 
    \uncover<7->{
    &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\epsilon_t} \nonumber
    }
    }
    }
    \end{align}
    \uncover<5-7>{Let $G_1 \sim \mathcal{N}(0,\sigma_1^2 I)$, $G_2 \sim \mathcal{N}(0,\sigma_2^2 I)$ , the sum of them gives $g_2 \sim \mathcal{N}(0,(\sigma_1^2 + \sigma_2^2) I)$.}
    }
    \only<8>{

        We have \alert{$x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1 - \bar{\alpha_t}} \epsilon$}.
    }
    }
\end{frame}

\begin{frame}{Training}
\only<1>{
For now, our model is learning $\mu$ and $\Sigma$, i.e. we sample according to 
\begin{align*}
p_\theta (x_{t-1} \mid x_t) := \mathcal{N}(x_{t-1} ; \mu_\theta(x_t,t), \Sigma_\theta (x_t,t))
\end{align*}
They've found that \alert{fixing $\Sigma_\theta$} to a constant gives the same result. So,}
\begin{align*}
    p_\theta (x_{t-1} \mid x_t) := \mathcal{N}(x_{t-1} ; \mu_\theta(x_t,t), \sigma_t I)
\end{align*}
\only<2->{
    The probability for our model to generate $x_0$ is $p_\theta (x_0) := \int p_\theta(x_{0:T}) dx_{1:T}$. \only<3->{Using negative log likelihood and computatios, we want to minimize:
    \begin{align*}
    E_q \left[\frac{1}{2 \sigma_t^2} \|\tilde{\mu}_t(x_t,x_0) - \mu_\theta(x_t,t)\|^2\right]
    \end{align*} where $\tilde{\mu}$ is the optimal mean \alert{that depends on $x_0$ \only<3>{which we don't know}}. \only<4>{Using $x_t(x_0,\epsilon) = \sqrt{\bar{\alpha_t}} x_0 + \sqrt{1 - \bar{\alpha_t}} \epsilon$ we \alert{have a loss we can train on}.}
    }
}
    
\end{frame}

\end{document}
